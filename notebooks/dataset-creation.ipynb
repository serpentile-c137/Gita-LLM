{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891e79be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d6a3fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token loaded: True\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(\"Token loaded:\", bool(GOOGLE_API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8128ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# INPUT_TEXT_FILE = \"gita_text.txt\"\n",
    "# OUTPUT_JSONL_FILE = \"gita_qa_dataset_per_verse.jsonl\"\n",
    "MAX_VERSES_TO_PROCESS = None # Set to a number (e.g., 10) for testing, or None for all\n",
    "MIN_VERSE_TEXT_LENGTH_WORDS = 30 # Minimum words for a verse's text to be considered for Q/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3041fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- API Key Setup ---\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    if GOOGLE_API_KEY is None:\n",
    "        print(\"Warning: GOOGLE_API_KEY not found in Colab userdata. Trying environment variable.\")\n",
    "        GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "except ImportError:\n",
    "    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    print(\"Error: GOOGLE_API_KEY not found. Please set it in Colab userdata or as an environment variable.\")\n",
    "    exit()\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d77f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gemini Model Configuration ---\n",
    "generation_config = {\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 2048,\n",
    "    \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "safety_settings = [\n",
    "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash-latest\",\n",
    "    safety_settings=safety_settings,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "138c6de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def load_text(filepath):\n",
    "    \"\"\"Loads text from a file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        exit()\n",
    "\n",
    "# User-provided parsing function\n",
    "def parse_gita_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses the Gita text to extract verses and their associated content.\n",
    "    Each item in the dataset will be a dictionary:\n",
    "    {\"chapter\": \"chapter_number\", \"verse\": \"verse_number\", \"text\": \"verse_content\"}\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    current_chapter = None\n",
    "    current_verse = None\n",
    "    buffer = []\n",
    "    # Keywords that often appear alone and should not be part of the main text buffer if they are the only content\n",
    "    standalone_structural_keywords = [\"SYNONYMS\", \"TRANSLATION\", \"PURPORT\"]\n",
    "\n",
    "\n",
    "    # Skip initial metadata lines\n",
    "    # Heuristic: assume metadata is at the very start. Find first \"Chapter 1\" or \"TEXT 1\"\n",
    "    start_processing = False\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            stripped = line.strip()\n",
    "\n",
    "            if not start_processing:\n",
    "                if re.match(r'^Chapter\\s+1', stripped, re.IGNORECASE) or \\\n",
    "                   re.match(r'^TEXT\\s+1', stripped, re.IGNORECASE) and current_chapter is None: # Ensure it's the first TEXT 1\n",
    "                    start_processing = True\n",
    "                else:\n",
    "                    continue # Skip metadata\n",
    "\n",
    "            chapter_match = re.match(r'^Chapter\\s+(\\d+)', stripped, re.IGNORECASE)\n",
    "            if chapter_match:\n",
    "                if buffer and current_chapter is not None and current_verse is not None:\n",
    "                    text_content = \" \".join(buffer).strip()\n",
    "                    if text_content: # Only add if there's actual text\n",
    "                        dataset.append({\n",
    "                            \"chapter\": current_chapter,\n",
    "                            \"verse\": current_verse,\n",
    "                            \"text\": text_content\n",
    "                        })\n",
    "                current_chapter = chapter_match.group(1)\n",
    "                current_verse = None # Reset verse when new chapter starts\n",
    "                buffer = []\n",
    "                continue\n",
    "\n",
    "            verse_match = re.match(r'^TEXT\\s+(\\d+)', stripped, re.IGNORECASE) # Made IGNORECASE more consistent\n",
    "            if verse_match:\n",
    "                if buffer and current_chapter is not None and current_verse is not None:\n",
    "                    text_content = \" \".join(buffer).strip()\n",
    "                    if text_content:\n",
    "                        dataset.append({\n",
    "                            \"chapter\": current_chapter,\n",
    "                            \"verse\": current_verse,\n",
    "                            \"text\": text_content\n",
    "                        })\n",
    "                current_verse = verse_match.group(1)\n",
    "                buffer = [] # Reset buffer for the new verse text\n",
    "                # We can choose to include the TEXT line itself or not.\n",
    "                # If not, we 'continue'. If yes, we let it fall through to buffer.append.\n",
    "                # For Q/A, probably better to exclude it from the chunk sent to Gemini.\n",
    "                continue # Skip adding \"TEXT X\" line to buffer\n",
    "\n",
    "            # Accumulate verse content only after a chapter and verse context is established\n",
    "            if current_chapter and current_verse:\n",
    "                # Avoid adding standalone keywords if they are the only thing on the line\n",
    "                if stripped and not (stripped in standalone_structural_keywords and len(stripped.split()) == 1):\n",
    "                     # Skip copyright lines that might appear at the end of chapters or sections\n",
    "                    if \"copyright\" in stripped.lower() or \"bhaktivedanta book\" in stripped.lower():\n",
    "                        continue\n",
    "                    if \"Thus end the Bhaktivedanta Purports\" in stripped: # Stop accumulating for this verse\n",
    "                        if buffer: # Add remaining buffer for the current verse\n",
    "                            text_content = \" \".join(buffer).strip()\n",
    "                            if text_content:\n",
    "                                dataset.append({\n",
    "                                    \"chapter\": current_chapter,\n",
    "                                    \"verse\": current_verse,\n",
    "                                    \"text\": text_content\n",
    "                                })\n",
    "                            buffer = []\n",
    "                            current_verse = None # Ready for next TEXT marker\n",
    "                        continue\n",
    "                    buffer.append(stripped)\n",
    "\n",
    "\n",
    "    # Add the last verse's content if any remains in the buffer\n",
    "    if buffer and current_chapter is not None and current_verse is not None:\n",
    "        text_content = \" \".join(buffer).strip()\n",
    "        if text_content:\n",
    "            dataset.append({\n",
    "                \"chapter\": current_chapter,\n",
    "                \"verse\": current_verse,\n",
    "                \"text\": text_content\n",
    "            })\n",
    "    \n",
    "    # Filter out entries with very short text\n",
    "    dataset = [item for item in dataset if len(item[\"text\"].split()) >= MIN_VERSE_TEXT_LENGTH_WORDS]\n",
    "    print(f\"Parsed {len(dataset)} verse entries meeting minimum length criteria.\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def generate_qa_from_verse_text(verse_item, retries=3, delay=5):\n",
    "    \"\"\"\n",
    "    Generates a Q/A pair from a verse's text content using Gemini.\n",
    "    Ensures output is valid JSON. verse_item is a dict with 'chapter', 'verse', 'text'.\n",
    "    \"\"\"\n",
    "    text_chunk = verse_item['text']\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in analyzing religious texts. Based ONLY on the following text, which is commentary on a verse from the \"Bhagavad-gita As It Is\", generate one insightful question and a concise, accurate answer.\n",
    "\n",
    "    Constraints:\n",
    "    1. The question MUST be answerable *solely* from the provided text.\n",
    "    2. The answer MUST be directly derivable and supported by the provided text.\n",
    "    3. Do NOT use any external knowledge or information not present in this specific text chunk.\n",
    "    4. Your response MUST be a single JSON object with two keys: \"question\" and \"answer\".\n",
    "    5. Ensure the question is about a significant aspect of the provided text.\n",
    "    6. The answer should be a direct summary or quote from the text that answers the question.\n",
    "\n",
    "    Example:\n",
    "    If the text is: \"The soul is eternal and cannot be destroyed by any weapon. It is unborn, ever-existing, and primeval.\"\n",
    "    A good JSON output would be:\n",
    "    {{\n",
    "      \"question\": \"According to this text, what are some key characteristics of the soul?\",\n",
    "      \"answer\": \"The text states that the soul is eternal, cannot be destroyed by any weapon, is unborn, ever-existing, and primeval.\"\n",
    "    }}\n",
    "\n",
    "    Provided Text (from Bhagavad-gita, Chapter {verse_item['chapter']}, Verse {verse_item['verse']}):\n",
    "    ---\n",
    "    {text_chunk}\n",
    "    ---\n",
    "\n",
    "    Your JSON Output:\n",
    "    \"\"\"\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            raw_json_text = response.text.strip()\n",
    "\n",
    "            match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", raw_json_text, re.DOTALL)\n",
    "            if match:\n",
    "                json_str = match.group(1)\n",
    "            else:\n",
    "                start_index = raw_json_text.find('{')\n",
    "                end_index = raw_json_text.rfind('}')\n",
    "                if start_index != -1 and end_index != -1 and end_index > start_index:\n",
    "                    json_str = raw_json_text[start_index : end_index + 1]\n",
    "                else:\n",
    "                    json_str = raw_json_text\n",
    "\n",
    "            try:\n",
    "                qa_pair = json.loads(json_str)\n",
    "                if \"question\" in qa_pair and \"answer\" in qa_pair:\n",
    "                    if qa_pair[\"question\"].strip() and qa_pair[\"answer\"].strip():\n",
    "                        return qa_pair\n",
    "                    else:\n",
    "                        print(f\"Warning: Generated Q/A has empty Q or A for C{verse_item['chapter']}:V{verse_item['verse']}.\")\n",
    "                else:\n",
    "                    print(f\"Warning: JSON missing 'question' or 'answer' for C{verse_item['chapter']}:V{verse_item['verse']}.\")\n",
    "                    print(f\"  Raw response part: {json_str[:200]}\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSONDecodeError (attempt {attempt + 1}) for C{verse_item['chapter']}:V{verse_item['verse']}: {e}\")\n",
    "                print(f\"  Problematic JSON string: {json_str[:200]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"API Error (attempt {attempt + 1}/{retries}) for C{verse_item['chapter']}:V{verse_item['verse']}: {e}\")\n",
    "\n",
    "        if attempt < retries - 1:\n",
    "            print(f\"Retrying in {delay * (attempt + 1)} seconds...\")\n",
    "            time.sleep(delay * (attempt + 1))\n",
    "        else:\n",
    "            print(f\"Failed to generate valid Q/A for C{verse_item['chapter']}:V{verse_item['verse']} after {retries} retries.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b69f6e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLEEP_BETWEEN_API_CALLS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9bc4c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter_04.txt', 'chapter_10.txt', 'chapter_11.txt', 'chapter_05.txt', 'chapter_13.txt', 'chapter_07.txt', 'chapter_06.txt', 'chapter_12.txt', 'chapter_16.txt', 'chapter_02.txt', 'chapter_03.txt', 'chapter_17.txt', 'chapter_01.txt', 'chapter_15.txt', 'chapter_14.txt', 'chapter_18.txt', 'chapter_08.txt', 'chapter_09.txt']\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"gita_chapters\"\n",
    "text_files = [f for f in os.listdir(directory_path) if f.endswith('.txt') and os.path.isfile(os.path.join(directory_path, f))]\n",
    "print(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe9522e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_TEXT_FILE='gita_text.txt'\n",
    "INPUT_TEXT_FILE='gita_chapters/chapter_02.txt'\n",
    "OUTPUT_JSONL_FILE = \"processed_data/gita_qa_dataset_per_verse_02.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be804ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 0 verse entries meeting minimum length criteria.\n",
      "No verse data parsed. Exiting.\n",
      "Found 0 verses to process.\n",
      "\n",
      "Starting Q/A generation for 0 verses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Q/A per verse: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 0 Q/A pairs.\n",
      "Dataset saved to processed_data/gita_qa_dataset_per_verse_02.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Main Script ---\n",
    "if __name__ == \"__main__\":\n",
    "    parsed_verses = parse_gita_text_from_file(INPUT_TEXT_FILE)\n",
    "\n",
    "    if not parsed_verses:\n",
    "        print(\"No verse data parsed. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    if MAX_VERSES_TO_PROCESS is not None and MAX_VERSES_TO_PROCESS > 0:\n",
    "        parsed_verses_to_process = parsed_verses[:MAX_VERSES_TO_PROCESS]\n",
    "        print(f\"Processing only the first {len(parsed_verses_to_process)} verses for testing.\")\n",
    "    else:\n",
    "        parsed_verses_to_process = parsed_verses\n",
    "        print(f\"Found {len(parsed_verses_to_process)} verses to process.\")\n",
    "\n",
    "\n",
    "    # Clear/Create the output file\n",
    "    with open(OUTPUT_JSONL_FILE, 'w', encoding='utf-8') as outfile:\n",
    "        pass # Ensures the file is new or cleared\n",
    "\n",
    "    generated_qa_count = 0\n",
    "    print(f\"\\nStarting Q/A generation for {len(parsed_verses_to_process)} verses...\")\n",
    "    for i, verse_item in enumerate(tqdm(parsed_verses_to_process, desc=\"Generating Q/A per verse\")):\n",
    "        \n",
    "        # The API call for Q/A generation happens within this function\n",
    "        qa_pair = generate_qa_from_verse_text(verse_item) \n",
    "\n",
    "        if qa_pair:\n",
    "            full_qa_entry = {\n",
    "                \"chapter\": verse_item['chapter'],\n",
    "                \"verse\": verse_item['verse'],\n",
    "                \"context\": verse_item['text'], # Optional: include the source text for traceability\n",
    "                \"question\": qa_pair['question'],\n",
    "                \"answer\": qa_pair['answer']\n",
    "            }\n",
    "            \n",
    "            with open(OUTPUT_JSONL_FILE, 'a', encoding='utf-8') as outfile:\n",
    "                json.dump(full_qa_entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "            generated_qa_count += 1\n",
    "        \n",
    "        # --- Sleep between API calls for different verses to respect rate limits ---\n",
    "        # This sleep is applied AFTER processing each verse (which includes one main API call).\n",
    "        if i < len(parsed_verses_to_process) - 1: # Don't sleep after the last item\n",
    "            time.sleep(SLEEP_BETWEEN_API_CALLS)\n",
    "\n",
    "    print(f\"\\nGenerated {generated_qa_count} Q/A pairs.\")\n",
    "    print(f\"Dataset saved to {OUTPUT_JSONL_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09d481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73fb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571009c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
